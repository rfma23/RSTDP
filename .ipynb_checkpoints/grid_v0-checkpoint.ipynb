{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic R-STDP Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an input for this basic R-STDP example we will give a `11x3` grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_dtype = np.dtype([('x', np.uint8), ('y', np.uint8), ('ts', np.float32)])\n",
    "c1_spike_dtype = np.dtype([('grid', np.uint8), ('y', np.uint8), ('x', np.uint8), ('ts', np.float32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid:\n",
    "    \"\"\"\n",
    "        Each grid represents a pixel array with spikes occurring at\n",
    "        a certain location at the given timestamp.\n",
    "    \"\"\"\n",
    "    def __init__(self, xsize, ysize, grid):\n",
    "        self.xsize = xsize\n",
    "        self.ysize = ysize\n",
    "        self.grid = grid\n",
    "        \n",
    "    @classmethod\n",
    "    def get_grid(cls, i):\n",
    "        \"\"\"\n",
    "            Allows easy initialization of a grid\n",
    "        \"\"\"\n",
    "        if i == 0 :\n",
    "            grid = np.array([[0,0,0,0,0,0,0,0,0,0,0],\n",
    "                             [0,0,0,0,0,0,0,0,0,0,0],\n",
    "                             [0,0,0,0,0,0,0,0,0,0,0]],\n",
    "                           dtype=np.float32)\n",
    "        elif i == 1 :\n",
    "            grid = np.array([[1,0,0,0,5,6,7,0,0,0,9],\n",
    "                             [2,3,0,0,0,8,0,0,0,10,11],\n",
    "                             [4,0,0,0,0,0,0,0,0,0,12]],\n",
    "                           dtype=np.float32)\n",
    "            \n",
    "        elif i == 2 :\n",
    "            grid = np.array([[1,0,0,0,9,10,11,0,0,0,5],\n",
    "                             [2,3,0,0,0,12,0,0,0,6,7],\n",
    "                             [4,0,0,0,0,0,0,0,0,0,8]],\n",
    "                           dtype=np.float32)\n",
    "        else:\n",
    "            raise ValueError('Not a supported grid type')\n",
    "            \n",
    "        return cls(11, 3, grid)\n",
    "    \n",
    "    def show(self):\n",
    "        \"\"\"\n",
    "            Prints a visual representation of the grid including \n",
    "            spike's timestamps\n",
    "        \"\"\"\n",
    "        grayscale = (self.grid > 0).astype(int)\n",
    "        for (j, i), value in np.ndenumerate(self.grid):\n",
    "            if value > 0 : plt.text(i, j, int(value))\n",
    "        plt.imshow(grayscale, vmin=-1, vmax =1, cmap='gray')\n",
    "        plt.xticks(range(self.grid.shape[1]), rotation=0)\n",
    "        plt.show()\n",
    "    \n",
    "    @property\n",
    "    def spikes(self):\n",
    "        \"\"\"\n",
    "            Retrieves the grid spikes in a recarray format with\n",
    "            spikes sorted by timestamp\n",
    "        \"\"\"\n",
    "        grid_spikes = []\n",
    "        for (j, i), value in np.ndenumerate(self.grid):\n",
    "             if value > 0 : grid_spikes.append((i, j, value))\n",
    "        np_spikes = np.array(grid_spikes, dtype=spike_dtype)\n",
    "        np_spikes.sort(order='ts')\n",
    "        return np_spikes\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.grid.shape\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.spikes}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = Grid.get_grid(1)\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([( 0, 0,  1.), ( 0, 1,  2.), ( 1, 1,  3.), ( 0, 2,  4.),\n",
       "       ( 4, 0,  5.), ( 5, 0,  6.), ( 6, 0,  7.), ( 5, 1,  8.),\n",
       "       (10, 0,  9.), ( 9, 1, 10.), (10, 1, 11.), (10, 2, 12.)],\n",
       "      dtype=[('x', 'u1'), ('y', 'u1'), ('ts', '<f4')])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = Grid.get_grid(2)\n",
    "b.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronal Grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neuronal grid has $n$ integrate-and-fire (IF) neurons with threshold $\\mathcal{T}$. Each neuron receives its inputs from a $\\omega \\times \\omega$ window -- also called the neuron's receptive field (RF) -- through weighted plastic synapses. In order to provide the ability of detecting a particular feature over the entire spatial positions, all the neurons belonging to the same grid share the same weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the arrival of a spike, we compute its relative position to a neuron $(x_r, y_r)$ and we increase the neuron's synaptic potential with the weight of the relative position if the spike is in its receptive field. In other words, the synaptic potential of neuron $i$ at time $t$ is: \n",
    "\n",
    "$$ v_i(t) = v_i(t-1) + \\sum_{(x_r, y_r) \\in {RF}}{w_{(x_r, y_r)} \\cdot \\delta(t-t_{spike}(j)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each neuronal grid detects a feature, the idea is to have multiple grids and associate each grid to a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "import itertools \n",
    "import logging\n",
    "#logging.basicConfig(filename='grid_mac.log',\n",
    "                    #format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "                    #level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronalGrids:\n",
    "    def __init__(self, input_grid_shape, neuron_rf=3, n_grids=4):\n",
    "        \n",
    "        self.input_grid_shape = input_grid_shape\n",
    "        self.grid_shape = ((input_grid_shape[0] - neuron_rf + 1),\n",
    "                           (input_grid_shape[1] - neuron_rf + 1))\n",
    "        \n",
    "        self.n_grids = n_grids\n",
    "        self.neuron_rf = neuron_rf\n",
    "        \n",
    "        self.synaptic_weights = self.initialise_weights(n_grids, neuron_rf)\n",
    "        self.neuron_potential = np.zeros((n_grids, *self.grid_shape), dtype=np.float32)\n",
    "        \n",
    "        self.sensitive_neurons_to_pos = self.get_sensitive_neurons()\n",
    "        self.neuron_threshold = 3.2\n",
    "        \n",
    "        self.reset_metrics()\n",
    "        self.Ar_plus = 0.075\n",
    "        self.Ar_neg = -0.05\n",
    "        self.Ap_plus = 0.04\n",
    "        self.Ap_neg = -0.1\n",
    "        \n",
    "        self.dropouts = []\n",
    "        \n",
    "        # to keep weights between [small_qty, 1-small_qty]\n",
    "        self.small_qty = 0.005\n",
    "    \n",
    "    def initialise_weights(self, n_grids, window_size, mu=0.8, sigma=0.05):\n",
    "        \"\"\"\n",
    "            Initialises the grid weights with values sampled from a normal\n",
    "            distribution mathcal{N}(mu, sigma2)\n",
    "        \"\"\"\n",
    "        # extract as many random samples as needed\n",
    "        grid = np.random.normal(mu, sigma, (n_grids, self.neuron_rf, self.neuron_rf))\n",
    "\n",
    "        # return them as a matrix\n",
    "        result = np.reshape(grid, (n_grids, self.neuron_rf, self.neuron_rf))\n",
    "        #result = np.full((n_grids, self.neuron_rf, self.neuron_rf), 0.8)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def reset_metrics(self):\n",
    "        self.n_silence = 0\n",
    "        self.n_hit = 0\n",
    "        self.n_miss = 0\n",
    "        self.n_samples = 0\n",
    "                \n",
    "    def get_temporal_winner(self, spikes, dropout=0.2, neuron_dropout=0.1):\n",
    "        \n",
    "        hasFired = False\n",
    "        is_active = np.random.random((self.n_grids))\n",
    "        is_active_neuron = np.random.random((self.grid_shape))\n",
    "        \n",
    "        for spike in spikes:\n",
    "            for grid in range(self.n_grids):\n",
    "                \n",
    "                if is_active[grid] > dropout:\n",
    "                    if not hasFired:\n",
    "\n",
    "                        affected_neurons = self.sensitive_neurons(spike)\n",
    "\n",
    "                        for neuron in affected_neurons:\n",
    "                            \n",
    "                            if is_active_neuron[neuron] > neuron_dropout:\n",
    "                                \n",
    "                                neuron_row, neuron_col = neuron\n",
    "                            \n",
    "                                relative_y, relative_x = self.relative_position(spike, neuron, self.neuron_rf)\n",
    "\n",
    "                                logging.info(f'Processing spike {spike}, Neuron {(neuron_row, neuron_col)}, Grid {grid}' + \\\n",
    "                                         f'Relative Pos {(relative_y, relative_x)}')\n",
    "                            \n",
    "                                self.neuron_potential[grid, neuron_row, neuron_col] += self.synaptic_weights[grid, relative_y, relative_x]\n",
    "\n",
    "                                if self.neuron_potential[grid, neuron_row, neuron_col] > self.neuron_threshold:\n",
    "                                    out_spike = np.array([(grid, neuron_row, neuron_col, spike['ts'])], dtype=c1_spike_dtype)\n",
    "                                    hasFired = True\n",
    "                                    break\n",
    "                                    \n",
    "                            # else ignore neuron\n",
    "\n",
    "                    else: # has fired\n",
    "                        return out_spike[0]\n",
    "                    \n",
    "                # else skip grid\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def predict(self, spikes):\n",
    "        \n",
    "        self.neuron_potential = np.zeros((self.n_grids, *self.grid_shape), dtype=np.float32)\n",
    "        winner_spike = self.get_temporal_winner(spikes, dropout=0, neuron_dropout=0)\n",
    "        \n",
    "        pred_class = None\n",
    "        \n",
    "        # if there is a winner: \n",
    "        if winner_spike is not None:\n",
    "            # get the class prediction of the winner\n",
    "            pred_class = winner_spike['grid'] % 2\n",
    "        \n",
    "        return pred_class\n",
    "        \n",
    "    def process(self, spikes, label, train=True):\n",
    "        \n",
    "        self.neuron_potential = np.zeros((self.n_grids, *self.grid_shape), dtype=np.float32)\n",
    "        \n",
    "        dropout=0.4*(1-self.n_hit/(self.n_samples+1))\n",
    "        neuron_dropout=0.2*(1-self.n_hit/(self.n_samples+1))\n",
    "        \n",
    "        self.dropouts.append((dropout, neuron_dropout))\n",
    "        \n",
    "        winner_spike = self.get_temporal_winner(spikes, dropout, neuron_dropout)\n",
    "        logging.info(winner_spike)\n",
    "        \n",
    "        # there can be no winners (no spikes), by default\n",
    "        reward = False\n",
    "        pred_class = None\n",
    "        \n",
    "        # if there is a winner: \n",
    "        if winner_spike is not None:\n",
    "            \n",
    "            # get the class prediction of the winner\n",
    "            pred_class = winner_spike['grid'] % 2\n",
    "            \n",
    "            # get the corresponding reward:\n",
    "            is_correct = (pred_class == label)\n",
    "            reward = is_correct\n",
    "            \n",
    "            logging.info(f'Reward {reward}, label {label}')\n",
    "            # compute metrics for performance and adjustment factor\n",
    "            self.n_hit += int(is_correct)\n",
    "            self.n_miss += int(is_correct)\n",
    "            \n",
    "            # if in training mode\n",
    "            if train:\n",
    "                # now we should trigger the learning process (only the winner grid updates its weights)\n",
    "                self.synaptic_plasticity(winner_spike, reward, spikes)\n",
    "        \n",
    "        # else there was no winner (silence)\n",
    "        else:\n",
    "            # increase all weights equally to the grids associated to the class\n",
    "            self.synaptic_weights[label] += 0.00005\n",
    "            # consider that for metrics\n",
    "            self.n_silence +=1\n",
    "            \n",
    "            # If none of the C2 neurons fire, no reward/punishment signal is generated, \n",
    "            # and thus, no weight change is applied.\n",
    "            \n",
    "                \n",
    "        # whatever the result, we processed one image\n",
    "        self.n_samples += 1\n",
    "        \n",
    "        return pred_class\n",
    "    \n",
    "    def synaptic_plasticity(self, winner_spike, reward, c1_spikes): \n",
    "        \n",
    "        \n",
    "        # self.synaptic_weights[winner_spike['grid']] contains the weights between C1 and S2, \n",
    "        # for a grid with one weight linking each orientation and (x,y) pixel\n",
    "        \n",
    "        grid_ix = winner_spike['grid']\n",
    "        \n",
    "        #if grid_ix==0: print(f\"Winner spike {winner_spike}, correct {reward}\")\n",
    "        # get whether there was a C1 spike relevant to neuron (in its receptive field)\n",
    "        # which spiked before the S2 spike\n",
    "        spiked_before_post = self.get_whether_pre_spiked_before_post(winner_spike, c1_spikes)\n",
    "        \n",
    "        # compute RSTDP update\n",
    "        computed_delta_weights = compute_RSTDP(self.synaptic_weights[grid_ix], spiked_before_post, reward, self.Ar_plus, self.Ar_neg, self.Ap_plus, self.Ap_neg)\n",
    "        \n",
    "        logging.info(f'Weights {self.synaptic_weights[grid_ix]}, Updates {computed_delta_weights}')\n",
    "        \n",
    "        # perform additive update rule with adaptive learning rate\n",
    "        if reward:\n",
    "            adjustment_factor = (self.n_miss+1)/(self.n_samples+1)\n",
    "        else:\n",
    "            adjustment_factor = (self.n_hit+1)/(self.n_samples+1)\n",
    "        \n",
    "        # apply the updates\n",
    "        apply_update(self.synaptic_weights[grid_ix], computed_delta_weights, adjustment_factor, self.small_qty)\n",
    "        \n",
    "    def relative_position(self, spike, neuron, neuron_rf):\n",
    "        row, col = neuron\n",
    "\n",
    "        relative_row = spike['y'] - row\n",
    "        relative_col = spike['x'] - col\n",
    "\n",
    "        return relative_row, relative_col\n",
    "    \n",
    "    \n",
    "    def get_sensitive_neurons(self):\n",
    "        \n",
    "        sensitive_neurons_to_pos = {}\n",
    "        for y, x in itertools.product(range(self.input_grid_shape[0]), range(self.input_grid_shape[1])) :\n",
    "            sensitive_neurons_to_pos[(y,x)] = []\n",
    "        \n",
    "        #print(sensitive_neurons_to_pos)\n",
    "        for row, col in itertools.product(range(self.grid_shape[0]), range(self.grid_shape[1])):\n",
    "            \n",
    "            neuron_center_row = row + self.neuron_rf//2\n",
    "            neuron_center_col = col + self.neuron_rf//2\n",
    "            \n",
    "            # relevant area for the neuron\n",
    "            row_lb = neuron_center_row-self.neuron_rf//2\n",
    "            row_ub = neuron_center_row+self.neuron_rf//2\n",
    "            col_lb = neuron_center_col-self.neuron_rf//2\n",
    "            col_ub = neuron_center_col+self.neuron_rf//2\n",
    "            \n",
    "            for y, x in itertools.product(range(row_lb, row_ub+1), range(col_lb, col_ub+1)):\n",
    "                sensitive_neurons_to_pos[(y,x)].append((row, col))\n",
    "                \n",
    "        return sensitive_neurons_to_pos\n",
    "                \n",
    "    def sensitive_neurons(self, spike):\n",
    "        return self.sensitive_neurons_to_pos[(spike['y'], spike['x'])]\n",
    "    \n",
    "    def get_whether_pre_spiked_before_post(self, s2_spike, c1_spikes):\n",
    "        \n",
    "        # we first need to understand which are the relevant rows and columns for an S2 neuron\n",
    "        # if a receptive field is 5x5 we have a relevant region in [0:4, 0:4] with center in 2,2\n",
    "        # this neuron centered in 2,2 is the S2 neuron (0,0), therefore:\n",
    "        \n",
    "        pre_spiked_before_post = np.full((self.neuron_rf, self.neuron_rf), False)\n",
    "        \n",
    "        neuron = (s2_spike['y'], s2_spike['x'])\n",
    "        \n",
    "        for spike in c1_spikes:\n",
    "            \n",
    "            #print(f\"Winner spike {s2_spike}, current spike {spike}\")\n",
    "            \n",
    "            if neuron in self.sensitive_neurons(spike):\n",
    "                relative_y, relative_x = self.relative_position(spike, neuron, self.neuron_rf)\n",
    "            \n",
    "                # and it spiked before\n",
    "                if(spike['ts'] <= s2_spike['ts']):\n",
    "                    \n",
    "                    #print(f\"Yeah it spiked before and relevant\")\n",
    "                    # we signal true:\n",
    "                    pre_spiked_before_post[relative_y, relative_x] = True\n",
    "                    \n",
    "               # else if silent or spiked later we keep it false\n",
    "            \n",
    "            # if not in receptive field we also keep it false\n",
    "            \n",
    "        return pre_spiked_before_post\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-STDP\n",
    "\n",
    "The weight updates are modulated by a reward/punishment signal which is received according to the correctness/incorrectness of the network's decision. The network's decision is given by the class associated to the grid which contains the first neuron to fire.\n",
    "\n",
    "If reward:\n",
    "$$\\Delta w_{ij} = \n",
    "\\begin{cases}\n",
    "a_r^+ \\times w_{ij} \\times (1-w_{ij}) & \\text{if } t^{f}_{j} \\leq t^{f}_{input} \\\\\n",
    "a_r^- \\times w_{ij} \\times (1-w_{ij}) &  \\text{if } t^{f}_{j} > t^{f}_{input} \\text{ or $j$ is silent }\\\\ \n",
    "\\end{cases}$$\n",
    "\n",
    "If punishment:\n",
    "$$\n",
    "\\Delta w_{ij} = \n",
    "\\begin{cases}\n",
    "a_p^+ \\times w_{ij} \\times (1-w_{ij}) &  \\text{if } t^{f}_{j} > t^{f}_{input} \\text{ or $j$ is silent }\\\\ \n",
    "a_p^- \\times w_{ij} \\times (1-w_{ij}) & \\text{if } t^{f}_{j} \\leq t^{f}_{input} \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "  \n",
    "**If none of the neurons in the grids fire, no reward/punishment signal is generated and thus no weight change is applied.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_RSTDP(synaptic_weights, pre_spiked_before_post, reward, Ar_plus, Ar_neg, Ap_plus, Ap_neg):\n",
    "    \"\"\"\n",
    "        Expects:\n",
    "            -  synaptic weights from a given grid\n",
    "            - a matrix indicating for each orientation row and column whether there was a spike\n",
    "            in that position firing before the winner spike\n",
    "            - reward or punishment\n",
    "            - parameters for RSTDP\n",
    "    \"\"\"\n",
    "    \n",
    "    # get C1-S2 synapses parameters\n",
    "    n_rows, n_cols = synaptic_weights.shape\n",
    "    \n",
    "    # initialize a container for the delta weights\n",
    "    delta_weights = np.zeros((n_rows, n_cols))\n",
    "    \n",
    "    # for each orientation, row and column (i.e. for each synaptic weight)\n",
    "    for (row, col), w_ij in np.ndenumerate(synaptic_weights):\n",
    "        \n",
    "        logging.info(f'Reward{reward}')\n",
    "        if reward: # apply normal STDP\n",
    "            \n",
    "            if pre_spiked_before_post[row,col]:\n",
    "                # correct decision, helpful neuron, boost weights so it reacts faster next time\n",
    "                delta_weights[row,col] = Ar_plus * synaptic_weights[row,col] * (1-synaptic_weights[row,col])\n",
    "                \n",
    "            else: # spiked after or silent\n",
    "                # correct decision, but not helpful spike, decrease weights ...\n",
    "                delta_weights[row,col] = Ar_neg * synaptic_weights[row,col] * (1-synaptic_weights[row,col])\n",
    "\n",
    "        else: # punishment signal reverses the polarity of STDP\n",
    "\n",
    "            if pre_spiked_before_post[row,col]:\n",
    "                # bad decision, decrease weights so we don't make the mistake next time\n",
    "                delta_weights[row,col] = Ap_neg * synaptic_weights[row,col] * (1-synaptic_weights[row,col])\n",
    "                \n",
    "            else: # spiked after or silent\n",
    "                # bad decision, but increase weights just to be sensitive to something else\n",
    "                delta_weights[row,col] = Ap_plus * synaptic_weights[row,col] * (1-synaptic_weights[row,col])\n",
    "    \n",
    "    return delta_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_update(synaptic_weights, computed_delta_weights, adjustment_factor, small_qty):\n",
    "    \"\"\" \n",
    "        Expects:\n",
    "            - synaptic weights from a given grid\n",
    "            - a delta weights computed with rstdp\n",
    "            - an adjustment factor acting as a learning rate\n",
    "            - a small quantity to keep weights between [small_qty, 1-small_qty]\n",
    "    \"\"\"\n",
    "\n",
    "    synaptic_weights += adjustment_factor * computed_delta_weights\n",
    "\n",
    "    # keep weights between 0 and 1\n",
    "    for (row, col), _ in np.ndenumerate(synaptic_weights):\n",
    "\n",
    "        if synaptic_weights[row, col] >= (1 - small_qty) : \n",
    "            synaptic_weights[row, col] = 1 - small_qty\n",
    "        elif synaptic_weights[row, col] <= 0 : \n",
    "            synaptic_weights[row, col] = 0 + small_qty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng = NeuronalGrids(a.shape, n_grids=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spike x=0, y=0\n",
      "\t Affected (0, 0), Relative position: x=[0] y=[0]\n",
      "Spike x=4, y=0\n",
      "\t Affected (0, 2), Relative position: x=[2] y=[0]\n",
      "\t Affected (0, 3), Relative position: x=[1] y=[0]\n",
      "\t Affected (0, 4), Relative position: x=[0] y=[0]\n",
      "Spike x=5, y=0\n",
      "\t Affected (0, 3), Relative position: x=[2] y=[0]\n",
      "\t Affected (0, 4), Relative position: x=[1] y=[0]\n",
      "\t Affected (0, 5), Relative position: x=[0] y=[0]\n",
      "Spike x=6, y=0\n",
      "\t Affected (0, 4), Relative position: x=[2] y=[0]\n",
      "\t Affected (0, 5), Relative position: x=[1] y=[0]\n",
      "\t Affected (0, 6), Relative position: x=[0] y=[0]\n",
      "Spike x=10, y=0\n",
      "\t Affected (0, 8), Relative position: x=[2] y=[0]\n",
      "Spike x=0, y=1\n",
      "\t Affected (0, 0), Relative position: x=[0] y=[1]\n",
      "Spike x=1, y=1\n",
      "\t Affected (0, 0), Relative position: x=[1] y=[1]\n",
      "\t Affected (0, 1), Relative position: x=[0] y=[1]\n",
      "Spike x=5, y=1\n",
      "\t Affected (0, 3), Relative position: x=[2] y=[1]\n",
      "\t Affected (0, 4), Relative position: x=[1] y=[1]\n",
      "\t Affected (0, 5), Relative position: x=[0] y=[1]\n",
      "Spike x=9, y=1\n",
      "\t Affected (0, 7), Relative position: x=[2] y=[1]\n",
      "\t Affected (0, 8), Relative position: x=[1] y=[1]\n",
      "Spike x=10, y=1\n",
      "\t Affected (0, 8), Relative position: x=[2] y=[1]\n",
      "Spike x=0, y=2\n",
      "\t Affected (0, 0), Relative position: x=[0] y=[2]\n",
      "Spike x=10, y=2\n",
      "\t Affected (0, 8), Relative position: x=[2] y=[2]\n"
     ]
    }
   ],
   "source": [
    "for (j,i), elem in np.ndenumerate(a.grid):\n",
    "    if elem > 0 : \n",
    "        print(f'Spike x={i}, y={j}')\n",
    "        affected_neurons = ng.sensitive_neurons_to_pos[(j,i)]\n",
    "        for neuron in affected_neurons:\n",
    "            relative_y, relative_x = ng.relative_position(np.array([(i, j, 0)], dtype=spike_dtype), neuron, 3)\n",
    "            print(f'\\t Affected {neuron}, Relative position: x={relative_x} y={relative_y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9105\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "n_iters = 2000\n",
    "# Train\n",
    "for i in range(n_iters):\n",
    "    rand_number = random.random()\n",
    "    if rand_number > 0.5 :\n",
    "        #print(\"\\nProcessing 0\")\n",
    "        pred = ng.process(a.spikes, 0)\n",
    "        correct += int(pred == 0) \n",
    "        logging.info('-'*20 + f'Predicted {pred}, real {0}')\n",
    "    else:\n",
    "        #print(\"\\nProcessing 1\")\n",
    "        pred = ng.process(b.spikes, 1)\n",
    "        correct += int(pred == 1) \n",
    "        logging.info('-'*20 + f'Predicted {pred}, real {1}')\n",
    "    \n",
    "print(f'Accuracy: {correct/(n_iters)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "correct = 0\n",
    "n_iters = 2000\n",
    "for i in range(n_iters):\n",
    "    rand_number = random.random()\n",
    "    if rand_number > 0.5 :\n",
    "        #print(\"\\nProcessing 0\")\n",
    "        pred = ng.predict(a.spikes)\n",
    "        correct += int(pred == 0) \n",
    "        logging.info('-'*20 + f'Predicted {pred}, real {0}')\n",
    "    else:\n",
    "        #print(\"\\nProcessing 1\")\n",
    "        pred = ng.predict(b.spikes)\n",
    "        correct += int(pred == 1) \n",
    "        logging.info('-'*20 + f'Predicted {pred}, real {1}')\n",
    "    \n",
    "print(f'Accuracy: {correct/(n_iters)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(len(ng.synaptic_weights)):\n",
    "    plt.imshow(ng.synaptic_weights[i], cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.05267868e-05, 3.05267868e-05, 9.95000000e-01],\n",
       "       [3.05267868e-05, 9.95000000e-01, 9.95000000e-01],\n",
       "       [3.05267868e-05, 3.05267868e-05, 9.95000000e-01]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng.synaptic_weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.4, 0.2),\n",
       " (0.4, 0.2),\n",
       " (0.4, 0.2),\n",
       " (0.30000000000000004, 0.15000000000000002),\n",
       " (0.32000000000000006, 0.16000000000000003),\n",
       " (0.33333333333333337, 0.16666666666666669),\n",
       " (0.28571428571428575, 0.14285714285714288),\n",
       " (0.30000000000000004, 0.15000000000000002),\n",
       " (0.3111111111111111, 0.15555555555555556),\n",
       " (0.32000000000000006, 0.16000000000000003),\n",
       " (0.29090909090909095, 0.14545454545454548),\n",
       " (0.30000000000000004, 0.15000000000000002),\n",
       " (0.27692307692307694, 0.13846153846153847),\n",
       " (0.28571428571428575, 0.14285714285714288),\n",
       " (0.2666666666666667, 0.13333333333333336),\n",
       " (0.275, 0.1375),\n",
       " (0.2823529411764706, 0.1411764705882353),\n",
       " (0.2666666666666667, 0.13333333333333336),\n",
       " (0.25263157894736843, 0.12631578947368421),\n",
       " (0.24, 0.12),\n",
       " (0.22857142857142856, 0.11428571428571428),\n",
       " (0.21818181818181817, 0.10909090909090909),\n",
       " (0.20869565217391306, 0.10434782608695653),\n",
       " (0.2, 0.1),\n",
       " (0.192, 0.096),\n",
       " (0.18461538461538463, 0.09230769230769231),\n",
       " (0.17777777777777778, 0.08888888888888889),\n",
       " (0.17142857142857146, 0.08571428571428573),\n",
       " (0.17931034482758623, 0.08965517241379312),\n",
       " (0.17333333333333334, 0.08666666666666667),\n",
       " (0.16774193548387095, 0.08387096774193548),\n",
       " (0.1625, 0.08125),\n",
       " (0.1575757575757576, 0.0787878787878788),\n",
       " (0.15294117647058825, 0.07647058823529412),\n",
       " (0.14857142857142858, 0.07428571428571429),\n",
       " (0.14444444444444446, 0.07222222222222223),\n",
       " (0.14054054054054052, 0.07027027027027026),\n",
       " (0.13684210526315788, 0.06842105263157894),\n",
       " (0.13333333333333336, 0.06666666666666668),\n",
       " (0.13999999999999999, 0.06999999999999999),\n",
       " (0.13658536585365852, 0.06829268292682926),\n",
       " (0.13333333333333336, 0.06666666666666668),\n",
       " (0.1302325581395349, 0.06511627906976745),\n",
       " (0.13636363636363638, 0.06818181818181819),\n",
       " (0.14222222222222222, 0.07111111111111111),\n",
       " (0.14782608695652175, 0.07391304347826087),\n",
       " (0.15319148936170213, 0.07659574468085106),\n",
       " (0.15000000000000002, 0.07500000000000001),\n",
       " (0.14693877551020407, 0.07346938775510203),\n",
       " (0.144, 0.072),\n",
       " (0.1411764705882353, 0.07058823529411765),\n",
       " (0.13846153846153847, 0.06923076923076923),\n",
       " (0.1358490566037736, 0.0679245283018868),\n",
       " (0.13333333333333336, 0.06666666666666668),\n",
       " (0.13818181818181818, 0.06909090909090909),\n",
       " (0.13571428571428573, 0.06785714285714287),\n",
       " (0.13333333333333336, 0.06666666666666668),\n",
       " (0.1310344827586207, 0.06551724137931035),\n",
       " (0.12881355932203392, 0.06440677966101696),\n",
       " (0.12666666666666668, 0.06333333333333334),\n",
       " (0.12459016393442623, 0.06229508196721312),\n",
       " (0.1225806451612903, 0.06129032258064515),\n",
       " (0.12063492063492065, 0.060317460317460325),\n",
       " (0.11875000000000001, 0.059375000000000004),\n",
       " (0.11692307692307691, 0.058461538461538454),\n",
       " (0.11515151515151514, 0.05757575757575757),\n",
       " (0.11343283582089554, 0.05671641791044777),\n",
       " (0.1176470588235294, 0.0588235294117647),\n",
       " (0.12173913043478263, 0.060869565217391314),\n",
       " (0.12000000000000002, 0.06000000000000001),\n",
       " (0.1183098591549296, 0.0591549295774648),\n",
       " (0.11666666666666665, 0.05833333333333333),\n",
       " (0.12054794520547946, 0.06027397260273973),\n",
       " (0.11891891891891891, 0.059459459459459456),\n",
       " (0.12266666666666666, 0.06133333333333333),\n",
       " (0.1210526315789474, 0.0605263157894737),\n",
       " (0.11948051948051948, 0.05974025974025974),\n",
       " (0.11794871794871793, 0.058974358974358966),\n",
       " (0.11645569620253166, 0.05822784810126583),\n",
       " (0.11499999999999999, 0.057499999999999996),\n",
       " (0.11358024691358026, 0.05679012345679013),\n",
       " (0.11219512195121953, 0.056097560975609764),\n",
       " (0.11084337349397591, 0.055421686746987955),\n",
       " (0.10952380952380954, 0.05476190476190477),\n",
       " (0.10823529411764708, 0.05411764705882354),\n",
       " (0.10697674418604652, 0.05348837209302326),\n",
       " (0.10574712643678162, 0.05287356321839081),\n",
       " (0.10454545454545455, 0.052272727272727276),\n",
       " (0.10337078651685393, 0.051685393258426963),\n",
       " (0.10222222222222221, 0.05111111111111111),\n",
       " (0.1010989010989011, 0.05054945054945055),\n",
       " (0.1, 0.05),\n",
       " (0.09892473118279571, 0.049462365591397855),\n",
       " (0.10212765957446811, 0.051063829787234054),\n",
       " (0.10105263157894738, 0.05052631578947369),\n",
       " (0.1, 0.05),\n",
       " (0.09896907216494846, 0.04948453608247423),\n",
       " (0.09795918367346941, 0.048979591836734705),\n",
       " (0.09696969696969698, 0.04848484848484849),\n",
       " (0.096, 0.048),\n",
       " (0.09504950495049505, 0.047524752475247525),\n",
       " (0.09411764705882356, 0.04705882352941178),\n",
       " (0.09320388349514563, 0.04660194174757282),\n",
       " (0.09615384615384617, 0.04807692307692309),\n",
       " (0.09523809523809526, 0.04761904761904763),\n",
       " (0.09433962264150941, 0.047169811320754707),\n",
       " (0.09345794392523366, 0.04672897196261683),\n",
       " (0.09629629629629628, 0.04814814814814814),\n",
       " (0.09908256880733944, 0.04954128440366972),\n",
       " (0.0981818181818182, 0.0490909090909091),\n",
       " (0.09729729729729729, 0.048648648648648644),\n",
       " (0.1, 0.05),\n",
       " (0.09911504424778764, 0.04955752212389382),\n",
       " (0.09824561403508772, 0.04912280701754386),\n",
       " (0.09739130434782611, 0.04869565217391306),\n",
       " (0.09655172413793106, 0.04827586206896553),\n",
       " (0.09914529914529915, 0.04957264957264958),\n",
       " (0.09830508474576272, 0.04915254237288136),\n",
       " (0.09747899159663867, 0.048739495798319335),\n",
       " (0.1, 0.05),\n",
       " (0.09917355371900825, 0.04958677685950413),\n",
       " (0.09836065573770494, 0.04918032786885247),\n",
       " (0.0975609756097561, 0.04878048780487805),\n",
       " (0.09677419354838711, 0.048387096774193554),\n",
       " (0.09920000000000001, 0.049600000000000005),\n",
       " (0.09841269841269842, 0.04920634920634921),\n",
       " (0.09763779527559056, 0.04881889763779528),\n",
       " (0.096875, 0.0484375),\n",
       " (0.09612403100775194, 0.04806201550387597),\n",
       " (0.09846153846153848, 0.04923076923076924),\n",
       " (0.09770992366412212, 0.04885496183206106),\n",
       " (0.09696969696969698, 0.04848484848484849),\n",
       " (0.0962406015037594, 0.0481203007518797),\n",
       " (0.09552238805970151, 0.047761194029850754),\n",
       " (0.0977777777777778, 0.0488888888888889),\n",
       " (0.09705882352941178, 0.04852941176470589),\n",
       " (0.09635036496350367, 0.04817518248175184),\n",
       " (0.09565217391304347, 0.047826086956521734),\n",
       " (0.0949640287769784, 0.0474820143884892),\n",
       " (0.0942857142857143, 0.04714285714285715),\n",
       " (0.09645390070921987, 0.048226950354609936),\n",
       " (0.09859154929577466, 0.04929577464788733),\n",
       " (0.09790209790209792, 0.04895104895104896),\n",
       " (0.09722222222222224, 0.04861111111111112),\n",
       " (0.09655172413793106, 0.04827586206896553),\n",
       " (0.0958904109589041, 0.04794520547945205),\n",
       " (0.09523809523809526, 0.04761904761904763),\n",
       " (0.09459459459459461, 0.04729729729729731),\n",
       " (0.09395973154362416, 0.04697986577181208),\n",
       " (0.096, 0.048),\n",
       " (0.09536423841059603, 0.04768211920529802),\n",
       " (0.09473684210526315, 0.047368421052631574),\n",
       " (0.09673202614379087, 0.04836601307189543),\n",
       " (0.09610389610389612, 0.04805194805194806),\n",
       " (0.09548387096774197, 0.047741935483870984),\n",
       " (0.0948717948717949, 0.04743589743589745),\n",
       " (0.09426751592356686, 0.04713375796178343),\n",
       " (0.09367088607594938, 0.04683544303797469),\n",
       " (0.09308176100628929, 0.046540880503144644),\n",
       " (0.09249999999999999, 0.04624999999999999),\n",
       " (0.09440993788819876, 0.04720496894409938),\n",
       " (0.09382716049382718, 0.04691358024691359),\n",
       " (0.09325153374233129, 0.046625766871165646),\n",
       " (0.09268292682926829, 0.046341463414634146),\n",
       " (0.09212121212121215, 0.04606060606060607),\n",
       " (0.09156626506024096, 0.04578313253012048),\n",
       " (0.0910179640718563, 0.04550898203592815),\n",
       " (0.09047619047619047, 0.04523809523809524),\n",
       " (0.08994082840236689, 0.044970414201183445),\n",
       " (0.09176470588235293, 0.045882352941176464),\n",
       " (0.09122807017543862, 0.04561403508771931),\n",
       " (0.09069767441860464, 0.04534883720930232),\n",
       " (0.0901734104046243, 0.04508670520231215),\n",
       " (0.09195402298850577, 0.04597701149425289),\n",
       " (0.09371428571428574, 0.04685714285714287),\n",
       " (0.09318181818181817, 0.046590909090909086),\n",
       " (0.09265536723163842, 0.04632768361581921),\n",
       " (0.09213483146067417, 0.04606741573033708),\n",
       " (0.09162011173184359, 0.045810055865921795),\n",
       " (0.0911111111111111, 0.04555555555555555),\n",
       " (0.09060773480662983, 0.045303867403314914),\n",
       " (0.09010989010989011, 0.045054945054945054),\n",
       " (0.09180327868852461, 0.04590163934426231),\n",
       " (0.09130434782608697, 0.045652173913043485),\n",
       " (0.09081081081081083, 0.045405405405405413),\n",
       " (0.0903225806451613, 0.04516129032258065),\n",
       " (0.08983957219251337, 0.044919786096256686),\n",
       " (0.09148936170212765, 0.045744680851063826),\n",
       " (0.09100529100529103, 0.04550264550264552),\n",
       " (0.0905263157894737, 0.04526315789473685),\n",
       " (0.0900523560209424, 0.0450261780104712),\n",
       " (0.08958333333333335, 0.044791666666666674),\n",
       " (0.08911917098445597, 0.044559585492227986),\n",
       " (0.088659793814433, 0.0443298969072165),\n",
       " (0.08820512820512821, 0.04410256410256411),\n",
       " (0.08775510204081632, 0.04387755102040816),\n",
       " (0.08934010152284265, 0.044670050761421325),\n",
       " (0.08888888888888889, 0.044444444444444446),\n",
       " (0.08844221105527637, 0.04422110552763819),\n",
       " (0.088, 0.044),\n",
       " (0.08756218905472637, 0.04378109452736319),\n",
       " (0.08712871287128712, 0.04356435643564356),\n",
       " (0.08669950738916254, 0.04334975369458127),\n",
       " (0.08627450980392158, 0.04313725490196079),\n",
       " (0.08585365853658536, 0.04292682926829268),\n",
       " (0.08543689320388351, 0.042718446601941754),\n",
       " (0.08502415458937196, 0.04251207729468598),\n",
       " (0.08461538461538463, 0.04230769230769232),\n",
       " (0.08421052631578947, 0.042105263157894736),\n",
       " (0.08380952380952383, 0.04190476190476192),\n",
       " (0.08341232227488155, 0.04170616113744077),\n",
       " (0.08301886792452828, 0.04150943396226414),\n",
       " (0.08262910798122065, 0.04131455399061033),\n",
       " (0.08224299065420562, 0.04112149532710281),\n",
       " (0.08372093023255817, 0.04186046511627908),\n",
       " (0.08333333333333336, 0.04166666666666668),\n",
       " (0.08294930875576037, 0.041474654377880185),\n",
       " (0.08256880733944955, 0.04128440366972477),\n",
       " (0.08219178082191783, 0.041095890410958916),\n",
       " (0.08181818181818185, 0.04090909090909092),\n",
       " (0.08144796380090501, 0.040723981900452504),\n",
       " (0.0810810810810811, 0.04054054054054055),\n",
       " (0.08251121076233187, 0.04125560538116593),\n",
       " (0.08214285714285717, 0.041071428571428585),\n",
       " (0.08177777777777778, 0.04088888888888889),\n",
       " (0.08141592920353982, 0.04070796460176991),\n",
       " (0.08105726872246698, 0.04052863436123349),\n",
       " (0.08245614035087719, 0.04122807017543859),\n",
       " (0.08209606986899565, 0.041048034934497823),\n",
       " (0.08173913043478263, 0.04086956521739132),\n",
       " (0.0813852813852814, 0.0406926406926407),\n",
       " (0.08103448275862069, 0.04051724137931034),\n",
       " (0.08068669527896995, 0.040343347639484974),\n",
       " (0.08034188034188033, 0.04017094017094017),\n",
       " (0.07999999999999999, 0.039999999999999994),\n",
       " (0.07966101694915256, 0.03983050847457628),\n",
       " (0.07932489451476794, 0.03966244725738397),\n",
       " (0.08067226890756302, 0.04033613445378151),\n",
       " (0.0803347280334728, 0.0401673640167364),\n",
       " (0.07999999999999999, 0.039999999999999994),\n",
       " (0.07966804979253111, 0.03983402489626556),\n",
       " (0.07933884297520662, 0.03966942148760331),\n",
       " (0.08065843621399177, 0.040329218106995884),\n",
       " (0.080327868852459, 0.0401639344262295),\n",
       " (0.0816326530612245, 0.04081632653061225),\n",
       " (0.08130081300813008, 0.04065040650406504),\n",
       " (0.08097165991902835, 0.04048582995951418),\n",
       " (0.0806451612903226, 0.0403225806451613),\n",
       " (0.0819277108433735, 0.04096385542168675),\n",
       " (0.08159999999999999, 0.040799999999999996),\n",
       " (0.0812749003984064, 0.0406374501992032),\n",
       " (0.08095238095238094, 0.04047619047619047),\n",
       " (0.08063241106719365, 0.04031620553359683),\n",
       " (0.08188976377952756, 0.04094488188976378),\n",
       " (0.08156862745098042, 0.04078431372549021),\n",
       " (0.08125, 0.040625),\n",
       " (0.08093385214007781, 0.040466926070038906),\n",
       " (0.0806201550387597, 0.04031007751937985),\n",
       " (0.08030888030888034, 0.04015444015444017),\n",
       " (0.07999999999999999, 0.039999999999999994),\n",
       " (0.07969348659003833, 0.039846743295019166),\n",
       " (0.07938931297709924, 0.03969465648854962),\n",
       " (0.07908745247148291, 0.039543726235741455),\n",
       " (0.07878787878787881, 0.039393939393939405),\n",
       " (0.07849056603773584, 0.03924528301886792),\n",
       " (0.07819548872180451, 0.039097744360902256),\n",
       " (0.07790262172284645, 0.03895131086142323),\n",
       " (0.07761194029850747, 0.038805970149253737),\n",
       " (0.07732342007434943, 0.03866171003717472),\n",
       " (0.07703703703703701, 0.03851851851851851),\n",
       " (0.07675276752767526, 0.03837638376383763),\n",
       " (0.07647058823529412, 0.03823529411764706),\n",
       " (0.0761904761904762, 0.0380952380952381),\n",
       " (0.07591240875912408, 0.03795620437956204),\n",
       " (0.07563636363636364, 0.03781818181818182),\n",
       " (0.0753623188405797, 0.03768115942028985),\n",
       " (0.07509025270758124, 0.03754512635379062),\n",
       " (0.07482014388489207, 0.037410071942446034),\n",
       " (0.07455197132616487, 0.037275985663082434),\n",
       " (0.07428571428571429, 0.037142857142857144),\n",
       " (0.07402135231316724, 0.03701067615658362),\n",
       " (0.07375886524822697, 0.03687943262411349),\n",
       " (0.07491166077738516, 0.03745583038869258),\n",
       " (0.07464788732394366, 0.03732394366197183),\n",
       " (0.07438596491228071, 0.037192982456140354),\n",
       " (0.07412587412587412, 0.03706293706293706),\n",
       " (0.07386759581881534, 0.03693379790940767),\n",
       " (0.07361111111111111, 0.03680555555555556),\n",
       " (0.07335640138408306, 0.03667820069204153),\n",
       " (0.07310344827586208, 0.03655172413793104),\n",
       " (0.07285223367697595, 0.036426116838487975),\n",
       " (0.07260273972602738, 0.03630136986301369),\n",
       " (0.07235494880546077, 0.036177474402730385),\n",
       " (0.07210884353741497, 0.03605442176870748),\n",
       " (0.07186440677966104, 0.03593220338983052),\n",
       " (0.07162162162162163, 0.03581081081081081),\n",
       " (0.07138047138047136, 0.03569023569023568),\n",
       " (0.07114093959731545, 0.035570469798657724),\n",
       " (0.07090301003344482, 0.03545150501672241),\n",
       " (0.07066666666666666, 0.03533333333333333),\n",
       " (0.07043189368770766, 0.03521594684385383),\n",
       " (0.07019867549668875, 0.035099337748344374),\n",
       " (0.07128712871287131, 0.035643564356435654),\n",
       " (0.0710526315789474, 0.0355263157894737),\n",
       " (0.07081967213114755, 0.035409836065573776),\n",
       " (0.07058823529411766, 0.03529411764705883),\n",
       " (0.07035830618892507, 0.03517915309446253),\n",
       " (0.07012987012987014, 0.03506493506493507),\n",
       " (0.06990291262135924, 0.03495145631067962),\n",
       " (0.07096774193548386, 0.03548387096774193),\n",
       " (0.0707395498392283, 0.03536977491961415),\n",
       " (0.07051282051282053, 0.03525641025641026),\n",
       " (0.07028753993610222, 0.03514376996805111),\n",
       " (0.07006369426751591, 0.035031847133757954),\n",
       " (0.06984126984126986, 0.03492063492063493),\n",
       " (0.07088607594936706, 0.03544303797468353),\n",
       " (0.07066246056782335, 0.03533123028391168),\n",
       " (0.07044025157232703, 0.035220125786163514),\n",
       " (0.07021943573667713, 0.035109717868338566),\n",
       " (0.07000000000000002, 0.03500000000000001),\n",
       " (0.06978193146417447, 0.034890965732087234),\n",
       " (0.06956521739130435, 0.034782608695652174),\n",
       " (0.0693498452012384, 0.0346749226006192),\n",
       " (0.0691358024691358, 0.0345679012345679),\n",
       " (0.0689230769230769, 0.03446153846153845),\n",
       " (0.0687116564417178, 0.0343558282208589),\n",
       " (0.0685015290519878, 0.0342507645259939),\n",
       " (0.06829268292682929, 0.034146341463414644),\n",
       " (0.06808510638297874, 0.03404255319148937),\n",
       " (0.06787878787878787, 0.033939393939393936),\n",
       " (0.0676737160120846, 0.0338368580060423),\n",
       " (0.06746987951807229, 0.033734939759036145),\n",
       " (0.06846846846846848, 0.03423423423423424),\n",
       " (0.0682634730538922, 0.0341317365269461),\n",
       " (0.0680597014925373, 0.03402985074626865),\n",
       " (0.06785714285714284, 0.03392857142857142),\n",
       " (0.06765578635014835, 0.033827893175074175),\n",
       " (0.06863905325443787, 0.03431952662721893),\n",
       " (0.06843657817109143, 0.034218289085545715),\n",
       " (0.06823529411764705, 0.03411764705882352),\n",
       " (0.06803519061583577, 0.03401759530791788),\n",
       " (0.06783625730994154, 0.03391812865497077),\n",
       " (0.0676384839650146, 0.0338192419825073),\n",
       " (0.06744186046511627, 0.033720930232558136),\n",
       " (0.0672463768115942, 0.0336231884057971),\n",
       " (0.06705202312138728, 0.03352601156069364),\n",
       " (0.06801152737752161, 0.03400576368876081),\n",
       " (0.06896551724137932, 0.03448275862068966),\n",
       " (0.06876790830945559, 0.034383954154727794),\n",
       " (0.06857142857142855, 0.034285714285714274),\n",
       " (0.06837606837606836, 0.03418803418803418),\n",
       " (0.06931818181818183, 0.03465909090909092),\n",
       " (0.07025495750708215, 0.03512747875354107),\n",
       " (0.07005649717514127, 0.035028248587570636),\n",
       " (0.06985915492957746, 0.03492957746478873),\n",
       " (0.0696629213483146, 0.0348314606741573),\n",
       " (0.06946778711484596, 0.03473389355742298),\n",
       " (0.06927374301675977, 0.034636871508379886),\n",
       " (0.06908077994428967, 0.034540389972144835),\n",
       " (0.06888888888888892, 0.03444444444444446),\n",
       " (0.06869806094182827, 0.03434903047091414),\n",
       " (0.06850828729281769, 0.034254143646408844),\n",
       " (0.06831955922865011, 0.03415977961432506),\n",
       " (0.06813186813186811, 0.034065934065934056),\n",
       " (0.06794520547945204, 0.03397260273972602),\n",
       " (0.06775956284153005, 0.033879781420765025),\n",
       " (0.06757493188010902, 0.03378746594005451),\n",
       " (0.06739130434782607, 0.033695652173913036),\n",
       " (0.06720867208672088, 0.03360433604336044),\n",
       " (0.06702702702702701, 0.033513513513513504),\n",
       " (0.06684636118598385, 0.03342318059299192),\n",
       " (0.06666666666666665, 0.033333333333333326),\n",
       " (0.06648793565683646, 0.03324396782841823),\n",
       " (0.06631016042780749, 0.033155080213903745),\n",
       " (0.06720000000000002, 0.03360000000000001),\n",
       " (0.06702127659574467, 0.03351063829787233),\n",
       " (0.06684350132625996, 0.03342175066312998),\n",
       " (0.06666666666666665, 0.033333333333333326),\n",
       " (0.06754617414248024, 0.03377308707124012),\n",
       " (0.06736842105263156, 0.03368421052631578),\n",
       " (0.06719160104986877, 0.03359580052493438),\n",
       " (0.0670157068062827, 0.03350785340314135),\n",
       " (0.06684073107049607, 0.033420365535248034),\n",
       " (0.06666666666666665, 0.033333333333333326),\n",
       " (0.06649350649350652, 0.03324675324675326),\n",
       " (0.06632124352331607, 0.033160621761658036),\n",
       " (0.06718346253229975, 0.03359173126614987),\n",
       " (0.0670103092783505, 0.03350515463917525),\n",
       " (0.06683804627249357, 0.033419023136246784),\n",
       " (0.06666666666666665, 0.033333333333333326),\n",
       " (0.06649616368286444, 0.03324808184143222),\n",
       " (0.06632653061224492, 0.03316326530612246),\n",
       " (0.06615776081424936, 0.03307888040712468),\n",
       " (0.06598984771573604, 0.03299492385786802),\n",
       " (0.06582278481012659, 0.03291139240506329),\n",
       " (0.06565656565656566, 0.03282828282828283),\n",
       " (0.06549118387909321, 0.032745591939546605),\n",
       " (0.06532663316582914, 0.03266331658291457),\n",
       " (0.06516290726817045, 0.032581453634085225),\n",
       " (0.06499999999999999, 0.032499999999999994),\n",
       " (0.06583541147132169, 0.03291770573566084),\n",
       " (0.06567164179104479, 0.032835820895522394),\n",
       " (0.06650124069478909, 0.033250620347394545),\n",
       " (0.06633663366336635, 0.03316831683168318),\n",
       " (0.06617283950617284, 0.03308641975308642),\n",
       " (0.06600985221674875, 0.033004926108374376),\n",
       " (0.06584766584766584, 0.03292383292383292),\n",
       " (0.06568627450980391, 0.032843137254901955),\n",
       " (0.06552567237163816, 0.03276283618581908),\n",
       " (0.0653658536585366, 0.0326829268292683),\n",
       " (0.06520681265206814, 0.03260340632603407),\n",
       " (0.0650485436893204, 0.0325242718446602),\n",
       " (0.06489104116222762, 0.03244552058111381),\n",
       " (0.06473429951690823, 0.03236714975845412),\n",
       " (0.06457831325301204, 0.03228915662650602),\n",
       " (0.06442307692307692, 0.03221153846153846),\n",
       " (0.06426858513189448, 0.03213429256594724),\n",
       " (0.06411483253588517, 0.03205741626794258),\n",
       " (0.0639618138424821, 0.03198090692124105),\n",
       " (0.0638095238095238, 0.0319047619047619),\n",
       " (0.06365795724465557, 0.03182897862232779),\n",
       " (0.06350710900473935, 0.03175355450236968),\n",
       " (0.06335697399527188, 0.03167848699763594),\n",
       " (0.06320754716981135, 0.031603773584905674),\n",
       " (0.06305882352941175, 0.031529411764705875),\n",
       " (0.06384976525821595, 0.031924882629107976),\n",
       " (0.06370023419203745, 0.031850117096018725),\n",
       " (0.06355140186915889, 0.031775700934579446),\n",
       " (0.06433566433566433, 0.032167832167832165),\n",
       " (0.06418604651162792, 0.03209302325581396),\n",
       " (0.06496519721577725, 0.03248259860788862),\n",
       " (0.06481481481481484, 0.03240740740740742),\n",
       " (0.06466512702078524, 0.03233256351039262),\n",
       " (0.06451612903225805, 0.032258064516129024),\n",
       " (0.06436781609195404, 0.03218390804597702),\n",
       " (0.06422018348623855, 0.032110091743119275),\n",
       " (0.06407322654462244, 0.03203661327231122),\n",
       " (0.06392694063926943, 0.031963470319634715),\n",
       " (0.06378132118451023, 0.031890660592255114),\n",
       " (0.06363636363636363, 0.031818181818181815),\n",
       " (0.06349206349206349, 0.031746031746031744),\n",
       " (0.06425339366515836, 0.03212669683257918),\n",
       " (0.06501128668171559, 0.032505643340857794),\n",
       " (0.06486486486486487, 0.032432432432432434),\n",
       " (0.06471910112359551, 0.032359550561797755),\n",
       " (0.06457399103139015, 0.032286995515695076),\n",
       " (0.06442953020134227, 0.03221476510067114),\n",
       " (0.06428571428571428, 0.03214285714285714),\n",
       " (0.06414253897550114, 0.03207126948775057),\n",
       " (0.06400000000000002, 0.03200000000000001),\n",
       " (0.06385809312638582, 0.03192904656319291),\n",
       " (0.06371681415929205, 0.031858407079646024),\n",
       " (0.06357615894039736, 0.03178807947019868),\n",
       " (0.06343612334801763, 0.03171806167400881),\n",
       " (0.0632967032967033, 0.03164835164835165),\n",
       " (0.06403508771929825, 0.032017543859649125),\n",
       " (0.0638949671772429, 0.03194748358862145),\n",
       " (0.06375545851528384, 0.03187772925764192),\n",
       " (0.06361655773420481, 0.031808278867102406),\n",
       " (0.0634782608695652, 0.0317391304347826),\n",
       " (0.06334056399132319, 0.031670281995661596),\n",
       " (0.06320346320346322, 0.03160173160173161),\n",
       " (0.06306695464362853, 0.03153347732181427),\n",
       " (0.06293103448275864, 0.03146551724137932),\n",
       " (0.06279569892473118, 0.03139784946236559),\n",
       " (0.06266094420600857, 0.031330472103004284),\n",
       " (0.06252676659528907, 0.03126338329764453),\n",
       " (0.062393162393162394, 0.031196581196581197),\n",
       " (0.06311300639658848, 0.03155650319829424),\n",
       " (0.06382978723404253, 0.03191489361702127),\n",
       " (0.06369426751592355, 0.031847133757961776),\n",
       " (0.06355932203389832, 0.03177966101694916),\n",
       " (0.06342494714587739, 0.031712473572938694),\n",
       " (0.06329113924050632, 0.03164556962025316),\n",
       " (0.06315789473684212, 0.03157894736842106),\n",
       " (0.06302521008403361, 0.031512605042016806),\n",
       " (0.06289308176100628, 0.03144654088050314),\n",
       " (0.06276150627615062, 0.03138075313807531),\n",
       " (0.0626304801670146, 0.0313152400835073),\n",
       " (0.06333333333333334, 0.03166666666666667),\n",
       " (0.0632016632016632, 0.0316008316008316),\n",
       " (0.06307053941908713, 0.03153526970954357),\n",
       " (0.06376811594202901, 0.031884057971014505),\n",
       " (0.06363636363636363, 0.031818181818181815),\n",
       " (0.06350515463917526, 0.03175257731958763),\n",
       " (0.06337448559670782, 0.03168724279835391),\n",
       " (0.06324435318275153, 0.03162217659137576),\n",
       " (0.06311475409836068, 0.03155737704918034),\n",
       " (0.06298568507157465, 0.031492842535787324),\n",
       " (0.06367346938775512, 0.03183673469387756),\n",
       " (0.0635437881873727, 0.03177189409368635),\n",
       " (0.06341463414634148, 0.03170731707317074),\n",
       " (0.06328600405679512, 0.03164300202839756),\n",
       " (0.06315789473684212, 0.03157894736842106),\n",
       " (0.06303030303030303, 0.03151515151515152),\n",
       " (0.06290322580645161, 0.031451612903225803),\n",
       " (0.06277665995975856, 0.03138832997987928),\n",
       " (0.06265060240963853, 0.031325301204819266),\n",
       " (0.06252505010020042, 0.03126252505010021),\n",
       " (0.06240000000000001, 0.031200000000000006),\n",
       " (0.062275449101796415, 0.031137724550898208),\n",
       " (0.06215139442231075, 0.031075697211155374),\n",
       " (0.06202783300198807, 0.031013916500994033),\n",
       " (0.06190476190476191, 0.030952380952380953),\n",
       " (0.061782178217821795, 0.030891089108910898),\n",
       " (0.061660079051383404, 0.030830039525691702),\n",
       " (0.06153846153846154, 0.03076923076923077),\n",
       " (0.061417322834645655, 0.030708661417322827),\n",
       " (0.0612966601178782, 0.0306483300589391),\n",
       " (0.061176470588235304, 0.030588235294117652),\n",
       " (0.06105675146771037, 0.030528375733855185),\n",
       " (0.060937500000000006, 0.030468750000000003),\n",
       " (0.0608187134502924, 0.0304093567251462),\n",
       " (0.06147859922178989, 0.030739299610894943),\n",
       " (0.06135922330097086, 0.03067961165048543),\n",
       " (0.0612403100775194, 0.0306201550387597),\n",
       " (0.06189555125725339, 0.030947775628626696),\n",
       " (0.06177606177606179, 0.030888030888030896),\n",
       " (0.06165703275529864, 0.03082851637764932),\n",
       " (0.06153846153846154, 0.03076923076923077),\n",
       " (0.0614203454894434, 0.0307101727447217),\n",
       " (0.06206896551724137, 0.031034482758620686),\n",
       " (0.06195028680688335, 0.030975143403441676),\n",
       " (0.061832061068702295, 0.030916030534351147),\n",
       " (0.0617142857142857, 0.03085714285714285),\n",
       " (0.062357414448669206, 0.031178707224334603),\n",
       " (0.06299810246679316, 0.03149905123339658),\n",
       " (0.0628787878787879, 0.03143939393939395),\n",
       " (0.0627599243856333, 0.03137996219281665),\n",
       " (0.06264150943396225, 0.031320754716981127),\n",
       " (0.06252354048964218, 0.03126177024482109),\n",
       " (0.06315789473684212, 0.03157894736842106),\n",
       " (0.06303939962476549, 0.03151969981238274),\n",
       " (0.06292134831460676, 0.03146067415730338),\n",
       " (0.06280373831775701, 0.031401869158878506),\n",
       " (0.06268656716417911, 0.03134328358208956),\n",
       " (0.06256983240223461, 0.031284916201117306),\n",
       " (0.062453531598512996, 0.031226765799256498),\n",
       " (0.06233766233766236, 0.03116883116883118),\n",
       " (0.06222222222222223, 0.031111111111111114),\n",
       " (0.0621072088724584, 0.0310536044362292),\n",
       " (0.06199261992619927, 0.030996309963099634),\n",
       " (0.06187845303867406, 0.03093922651933703),\n",
       " (0.061764705882352944, 0.030882352941176472),\n",
       " (0.061651376146789, 0.0308256880733945),\n",
       " (0.06153846153846154, 0.03076923076923077),\n",
       " (0.061425959780621577, 0.030712979890310788),\n",
       " (0.0613138686131387, 0.03065693430656935),\n",
       " (0.061202185792349706, 0.030601092896174853),\n",
       " (0.061090909090909085, 0.030545454545454542),\n",
       " (0.06098003629764066, 0.03049001814882033),\n",
       " (0.060869565217391314, 0.030434782608695657),\n",
       " (0.060759493670886094, 0.030379746835443047),\n",
       " (0.06064981949458486, 0.03032490974729243),\n",
       " (0.06054054054054055, 0.030270270270270273),\n",
       " (0.060431654676258974, 0.030215827338129487),\n",
       " (0.061041292639138246, 0.030520646319569123),\n",
       " (0.06093189964157708, 0.03046594982078854),\n",
       " (0.06082289803220036, 0.03041144901610018),\n",
       " (0.06071428571428572, 0.03035714285714286),\n",
       " (0.0606060606060606, 0.0303030303030303),\n",
       " (0.0604982206405694, 0.0302491103202847),\n",
       " (0.060390763765541734, 0.030195381882770867),\n",
       " (0.0602836879432624, 0.0301418439716312),\n",
       " (0.060176991150442484, 0.030088495575221242),\n",
       " (0.060070671378091856, 0.030035335689045928),\n",
       " (0.05996472663139332, 0.02998236331569666),\n",
       " (0.059859154929577454, 0.029929577464788727),\n",
       " (0.05975395430579966, 0.02987697715289983),\n",
       " (0.059649122807017556, 0.029824561403508778),\n",
       " (0.05954465849387041, 0.029772329246935205),\n",
       " (0.05944055944055946, 0.02972027972027973),\n",
       " (0.05933682373472951, 0.029668411867364755),\n",
       " (0.05923344947735192, 0.02961672473867596),\n",
       " (0.05913043478260871, 0.029565217391304355),\n",
       " (0.05902777777777777, 0.029513888888888885),\n",
       " (0.05892547660311958, 0.02946273830155979),\n",
       " (0.05882352941176472, 0.02941176470588236),\n",
       " (0.05872193436960278, 0.02936096718480139),\n",
       " (0.05862068965517242, 0.02931034482758621),\n",
       " (0.058519793459552494, 0.029259896729776247),\n",
       " (0.05841924398625431, 0.029209621993127155),\n",
       " (0.05831903945111492, 0.02915951972555746),\n",
       " (0.05821917808219177, 0.029109589041095885),\n",
       " (0.058119658119658135, 0.029059829059829068),\n",
       " (0.05802047781569968, 0.02901023890784984),\n",
       " (0.05792163543441227, 0.028960817717206135),\n",
       " (0.058503401360544244, 0.029251700680272122),\n",
       " (0.05840407470288627, 0.029202037351443135),\n",
       " (0.05830508474576273, 0.029152542372881365),\n",
       " (0.058206429780033854, 0.029103214890016927),\n",
       " (0.05810810810810812, 0.02905405405405406),\n",
       " (0.058010118043844865, 0.029005059021922432),\n",
       " (0.057912457912457915, 0.028956228956228958),\n",
       " (0.05848739495798321, 0.029243697478991606),\n",
       " (0.05838926174496644, 0.02919463087248322),\n",
       " (0.05829145728643215, 0.029145728643216073),\n",
       " (0.058193979933110374, 0.029096989966555187),\n",
       " (0.05809682804674457, 0.029048414023372285),\n",
       " (0.05800000000000001, 0.029000000000000005),\n",
       " (0.05790349417637271, 0.028951747088186354),\n",
       " (0.05780730897009967, 0.028903654485049834),\n",
       " (0.057711442786069655, 0.028855721393034828),\n",
       " (0.057615894039735105, 0.028807947019867552),\n",
       " (0.05752066115702479, 0.028760330578512395),\n",
       " (0.057425742574257435, 0.028712871287128718),\n",
       " (0.05733113673805601, 0.028665568369028005),\n",
       " (0.05723684210526314, 0.02861842105263157),\n",
       " (0.05714285714285716, 0.02857142857142858),\n",
       " (0.057049180327868855, 0.028524590163934428),\n",
       " (0.056955810147299515, 0.028477905073649758),\n",
       " (0.056862745098039236, 0.028431372549019618),\n",
       " (0.05676998368678632, 0.02838499184339316),\n",
       " (0.05667752442996741, 0.028338762214983704),\n",
       " (0.056585365853658545, 0.028292682926829273),\n",
       " (0.05649350649350651, 0.028246753246753256),\n",
       " (0.05705024311183143, 0.028525121555915713),\n",
       " (0.056957928802588986, 0.028478964401294493),\n",
       " (0.05686591276252018, 0.02843295638126009),\n",
       " (0.056774193548387114, 0.028387096774193557),\n",
       " (0.05668276972624797, 0.028341384863123986),\n",
       " (0.05659163987138265, 0.028295819935691326),\n",
       " (0.05650080256821828, 0.02825040128410914),\n",
       " (0.05641025641025643, 0.028205128205128216),\n",
       " (0.056320000000000016, 0.028160000000000008),\n",
       " (0.05623003194888181, 0.028115015974440907),\n",
       " (0.056140350877192984, 0.028070175438596492),\n",
       " (0.056050955414012726, 0.028025477707006363),\n",
       " (0.0559618441971383, 0.02798092209856915),\n",
       " (0.05587301587301585, 0.027936507936507926),\n",
       " (0.055784469096671965, 0.027892234548335983),\n",
       " (0.05632911392405063, 0.028164556962025314),\n",
       " (0.056240126382306466, 0.028120063191153233),\n",
       " (0.05615141955835963, 0.028075709779179815),\n",
       " (0.05606299212598427, 0.028031496062992135),\n",
       " (0.05597484276729561, 0.027987421383647806),\n",
       " (0.05588697017268447, 0.027943485086342236),\n",
       " (0.05579937304075236, 0.02789968652037618),\n",
       " (0.05571205007824727, 0.027856025039123634),\n",
       " (0.055624999999999994, 0.027812499999999997),\n",
       " (0.05553822152886117, 0.027769110764430584),\n",
       " (0.05545171339563862, 0.02772585669781931),\n",
       " (0.05536547433903576, 0.02768273716951788),\n",
       " (0.05527950310559007, 0.027639751552795036),\n",
       " (0.05519379844961239, 0.027596899224806196),\n",
       " (0.05510835913312695, 0.027554179566563475),\n",
       " (0.05502318392581143, 0.027511591962905715),\n",
       " (0.0549382716049383, 0.02746913580246915),\n",
       " (0.054853620955315874, 0.027426810477657937),\n",
       " (0.05476923076923077, 0.027384615384615386),\n",
       " (0.05468509984639019, 0.027342549923195094),\n",
       " (0.05460122699386503, 0.027300613496932514),\n",
       " (0.0545176110260337, 0.02725880551301685),\n",
       " (0.054434250764525995, 0.027217125382262997),\n",
       " (0.05435114503816796, 0.02717557251908398),\n",
       " (0.054268292682926815, 0.027134146341463408),\n",
       " (0.05418569254185695, 0.027092846270928474),\n",
       " (0.05410334346504558, 0.02705167173252279),\n",
       " (0.05402124430955993, 0.027010622154779965),\n",
       " (0.053939393939393954, 0.026969696969696977),\n",
       " (0.053857791225416035, 0.026928895612708018),\n",
       " (0.05438066465256797, 0.027190332326283984),\n",
       " (0.05429864253393664, 0.02714932126696832),\n",
       " (0.054216867469879526, 0.027108433734939763),\n",
       " (0.05413533834586466, 0.02706766917293233),\n",
       " (0.054054054054054036, 0.027027027027027018),\n",
       " (0.054572713643178394, 0.027286356821589197),\n",
       " (0.05449101796407185, 0.027245508982035926),\n",
       " (0.054409566517189846, 0.027204783258594923),\n",
       " (0.054328358208955235, 0.027164179104477618),\n",
       " (0.054247391952309966, 0.027123695976154983),\n",
       " (0.054166666666666655, 0.027083333333333327),\n",
       " (0.054086181277860315, 0.027043090638930158),\n",
       " (0.05400593471810092, 0.02700296735905046),\n",
       " (0.05392592592592593, 0.026962962962962966),\n",
       " (0.053846153846153835, 0.026923076923076918),\n",
       " (0.05376661742983751, 0.026883308714918754),\n",
       " (0.05427728613569323, 0.027138643067846614),\n",
       " (0.054197349042709855, 0.027098674521354928),\n",
       " (0.05411764705882352, 0.02705882352941176),\n",
       " (0.05403817914831133, 0.027019089574155664),\n",
       " (0.05395894428152492, 0.02697947214076246),\n",
       " (0.05387994143484627, 0.026939970717423135),\n",
       " (0.053801169590643294, 0.026900584795321647),\n",
       " (0.053722627737226296, 0.026861313868613148),\n",
       " (0.05364431486880466, 0.02682215743440233),\n",
       " (0.05414847161572052, 0.02707423580786026),\n",
       " (0.054069767441860477, 0.027034883720930238),\n",
       " (0.05399129172714079, 0.026995645863570394),\n",
       " (0.05391304347826087, 0.026956521739130435),\n",
       " (0.05383502170767005, 0.026917510853835026),\n",
       " (0.05375722543352599, 0.026878612716762996),\n",
       " (0.05367965367965369, 0.026839826839826844),\n",
       " (0.05360230547550433, 0.026801152737752167),\n",
       " (0.053525179856115115, 0.026762589928057558),\n",
       " (0.053448275862068954, 0.026724137931034477),\n",
       " (0.05337159253945481, 0.026685796269727405),\n",
       " (0.05329512893982807, 0.026647564469914033),\n",
       " (0.05379113018597997, 0.026895565092989984),\n",
       " (0.053714285714285694, 0.026857142857142847),\n",
       " (0.0542082738944365, 0.02710413694721825),\n",
       " (0.05413105413105415, 0.027065527065527076),\n",
       " (0.054054054054054036, 0.027027027027027018),\n",
       " (0.05454545454545454, 0.02727272727272727),\n",
       " (0.05446808510638297, 0.027234042553191486),\n",
       " (0.05439093484419262, 0.02719546742209631),\n",
       " (0.054879773691654865, 0.027439886845827433),\n",
       " (0.05480225988700567, 0.027401129943502835),\n",
       " (0.05472496473906912, 0.02736248236953456),\n",
       " (0.05521126760563382, 0.02760563380281691),\n",
       " (0.05513361462728553, 0.027566807313642763),\n",
       " (0.05505617977528088, 0.02752808988764044),\n",
       " (0.05497896213183733, 0.027489481065918666),\n",
       " (0.05490196078431371, 0.027450980392156855),\n",
       " (0.05482517482517482, 0.02741258741258741),\n",
       " (0.054748603351955305, 0.027374301675977653),\n",
       " (0.05467224546722456, 0.02733612273361228),\n",
       " (0.05459610027855155, 0.027298050139275776),\n",
       " (0.054520166898470106, 0.027260083449235053),\n",
       " (0.054444444444444434, 0.027222222222222217),\n",
       " (0.05436893203883497, 0.027184466019417486),\n",
       " (0.054293628808864285, 0.027146814404432142),\n",
       " (0.05421853388658367, 0.027109266943291833),\n",
       " (0.0541436464088398, 0.0270718232044199),\n",
       " (0.054068965517241364, 0.027034482758620682),\n",
       " (0.053994490358126736, 0.026997245179063368),\n",
       " (0.05392022008253097, 0.026960110041265486),\n",
       " (0.053846153846153835, 0.026923076923076918),\n",
       " (0.05377229080932784, 0.02688614540466392),\n",
       " (0.054246575342465776, 0.027123287671232888),\n",
       " (0.05417236662106704, 0.02708618331053352),\n",
       " (0.05464480874316942, 0.02732240437158471),\n",
       " (0.054570259208731244, 0.027285129604365622),\n",
       " (0.054495912806539516, 0.027247956403269758),\n",
       " (0.05442176870748301, 0.027210884353741506),\n",
       " (0.054891304347826075, 0.027445652173913038),\n",
       " (0.05481682496607872, 0.02740841248303936),\n",
       " (0.05474254742547427, 0.027371273712737135),\n",
       " (0.054668470906630566, 0.027334235453315283),\n",
       " (0.05459459459459462, 0.02729729729729731),\n",
       " (0.0545209176788124, 0.0272604588394062),\n",
       " (0.05444743935309973, 0.027223719676549865),\n",
       " (0.054912516823687745, 0.027456258411843872),\n",
       " (0.054838709677419356, 0.027419354838709678),\n",
       " (0.05476510067114093, 0.027382550335570466),\n",
       " (0.05469168900804289, 0.027345844504021444),\n",
       " (0.054618473895582345, 0.027309236947791173),\n",
       " (0.05454545454545454, 0.02727272727272727),\n",
       " (0.05447263017356474, 0.02723631508678237),\n",
       " (0.054400000000000004, 0.027200000000000002),\n",
       " (0.05432756324900132, 0.02716378162450066),\n",
       " (0.05425531914893616, 0.02712765957446808),\n",
       " (0.0547144754316069, 0.02735723771580345),\n",
       " (0.0546419098143236, 0.0273209549071618),\n",
       " (0.05456953642384108, 0.02728476821192054),\n",
       " (0.05449735449735452, 0.02724867724867726),\n",
       " (0.05442536327608982, 0.02721268163804491),\n",
       " (0.05435356200527704, 0.02717678100263852),\n",
       " (0.05428194993412383, 0.027140974967061915),\n",
       " (0.05421052631578949, 0.027105263157894743),\n",
       " (0.05413929040735872, 0.02706964520367936),\n",
       " (0.054068241469816286, 0.027034120734908143),\n",
       " (0.05399737876802098, 0.02699868938401049),\n",
       " (0.05392670157068063, 0.026963350785340314),\n",
       " (0.05385620915032679, 0.026928104575163394),\n",
       " (0.0537859007832898, 0.0268929503916449),\n",
       " (0.05371577574967406, 0.02685788787483703),\n",
       " (0.05364583333333335, 0.026822916666666675),\n",
       " (0.05357607282184654, 0.02678803641092327),\n",
       " (0.0535064935064935, 0.02675324675324675),\n",
       " (0.05343709468223086, 0.02671854734111543),\n",
       " (0.05336787564766841, 0.026683937823834204),\n",
       " (0.05329883570504528, 0.02664941785252264),\n",
       " (0.05322997416020674, 0.02661498708010337),\n",
       " (0.05316129032258066, 0.02658064516129033),\n",
       " (0.05309278350515463, 0.026546391752577315),\n",
       " (0.05302445302445302, 0.02651222651222651),\n",
       " (0.05295629820051415, 0.026478149100257076),\n",
       " (0.05288831835686776, 0.02644415917843388),\n",
       " (0.05282051282051281, 0.026410256410256405),\n",
       " (0.052752880921894986, 0.026376440460947493),\n",
       " (0.05268542199488491, 0.026342710997442453),\n",
       " (0.05261813537675608, 0.02630906768837804),\n",
       " (0.052551020408163264, 0.026275510204081632),\n",
       " (0.05248407643312101, 0.026242038216560504),\n",
       " (0.052417302798982185, 0.026208651399491092),\n",
       " (0.05235069885641677, 0.026175349428208386),\n",
       " (0.05228426395939088, 0.02614213197969544),\n",
       " (0.05221799746514577, 0.026108998732572886),\n",
       " (0.05215189873417723, 0.026075949367088614),\n",
       " (0.05208596713021492, 0.02604298356510746),\n",
       " (0.052020202020202036, 0.026010101010101018),\n",
       " (0.05195460277427491, 0.025977301387137455),\n",
       " (0.05188916876574306, 0.02594458438287153),\n",
       " (0.051823899371069175, 0.025911949685534588),\n",
       " (0.052261306532663324, 0.026130653266331662),\n",
       " (0.05219573400250943, 0.026097867001254715),\n",
       " (0.05213032581453634, 0.02606516290726817),\n",
       " (0.05206508135168959, 0.026032540675844797),\n",
       " (0.052000000000000005, 0.026000000000000002),\n",
       " (0.051935081148564294, 0.025967540574282147),\n",
       " (0.051870324189526196, 0.025935162094763098),\n",
       " (0.05180572851805727, 0.025902864259028636),\n",
       " (0.0517412935323383, 0.02587064676616915),\n",
       " (0.05167701863354038, 0.02583850931677019),\n",
       " (0.05161290322580645, 0.025806451612903226),\n",
       " (0.05154894671623298, 0.02577447335811649),\n",
       " (0.05148514851485149, 0.025742574257425745),\n",
       " (0.05142150803461063, 0.025710754017305316),\n",
       " (0.05135802469135804, 0.02567901234567902),\n",
       " (0.05129469790382246, 0.02564734895191123),\n",
       " (0.051231527093596047, 0.025615763546798023),\n",
       " (0.05116851168511687, 0.025584255842558436),\n",
       " (0.05110565110565109, 0.025552825552825544),\n",
       " (0.051042944785276094, 0.025521472392638047),\n",
       " (0.051470588235294115, 0.025735294117647058),\n",
       " (0.051407588739290105, 0.025703794369645053),\n",
       " (0.051344743276283605, 0.025672371638141803),\n",
       " (0.05128205128205128, 0.02564102564102564),\n",
       " (0.051219512195121955, 0.025609756097560978),\n",
       " (0.05164433617539586, 0.02582216808769793),\n",
       " (0.05158150851581507, 0.025790754257907535),\n",
       " (0.05151883353584448, 0.02575941676792224),\n",
       " (0.05145631067961168, 0.02572815533980584),\n",
       " (0.051393939393939415, 0.025696969696969708),\n",
       " (0.0513317191283293, 0.02566585956416465),\n",
       " (0.0512696493349456, 0.0256348246674728),\n",
       " (0.05120772946859904, 0.02560386473429952),\n",
       " (0.051145958986731, 0.0255729794933655),\n",
       " (0.051084337349397574, 0.025542168674698787),\n",
       " (0.05102286401925391, 0.025511432009626956),\n",
       " (0.05096153846153846, 0.02548076923076923),\n",
       " (0.05090036014405763, 0.025450180072028816),\n",
       " (0.05083932853717026, 0.02541966426858513),\n",
       " (0.05077844311377247, 0.025389221556886235),\n",
       " (0.05071770334928232, 0.02535885167464116),\n",
       " (0.050657108721624856, 0.025328554360812428),\n",
       " (0.05059665871121717, 0.025298329355608586),\n",
       " (0.050536352800953525, 0.025268176400476763),\n",
       " (0.05047619047619048, 0.02523809523809524),\n",
       " (0.05041617122473246, 0.02520808561236623),\n",
       " (0.050356294536817094, 0.025178147268408547),\n",
       " (0.050296559905100846, 0.025148279952550423),\n",
       " (0.05023696682464456, 0.02511848341232228),\n",
       " (0.050177514792899426, 0.025088757396449713),\n",
       " (0.05011820330969266, 0.02505910165484633),\n",
       " (0.0500590318772137, 0.02502951593860685),\n",
       " (0.05, 0.025),\n",
       " (0.04994110718492344, 0.02497055359246172),\n",
       " (0.04988235294117649, 0.024941176470588244),\n",
       " (0.04982373678025853, 0.024911868390129266),\n",
       " (0.04976525821596245, 0.024882629107981225),\n",
       " (0.04970691676436108, 0.02485345838218054),\n",
       " (0.04964871194379392, 0.02482435597189696),\n",
       " (0.04959064327485381, 0.024795321637426905),\n",
       " (0.04953271028037385, 0.024766355140186925),\n",
       " (0.049474912485414226, 0.024737456242707113),\n",
       " (0.04941724941724943, 0.024708624708624716),\n",
       " (0.049359720605355054, 0.024679860302677527),\n",
       " (0.049302325581395356, 0.024651162790697678),\n",
       " (0.04924506387921022, 0.02462253193960511),\n",
       " (0.04965197215777262, 0.02482598607888631),\n",
       " (0.04959443800695249, 0.024797219003476246),\n",
       " (0.049537037037037025, 0.024768518518518513),\n",
       " (0.04947976878612717, 0.024739884393063585),\n",
       " (0.04942263279445727, 0.024711316397228636),\n",
       " (0.04936562860438292, 0.02468281430219146),\n",
       " (0.04930875576036869, 0.024654377880184344),\n",
       " (0.049252013808975814, 0.024626006904487907),\n",
       " (0.049655172413793115, 0.024827586206896558),\n",
       " (0.04959816303099887, 0.024799081515499435),\n",
       " (0.05, 0.025),\n",
       " (0.04994272623138603, 0.024971363115693014),\n",
       " (0.04988558352402746, 0.02494279176201373),\n",
       " (0.04982857142857142, 0.02491428571428571),\n",
       " (0.04977168949771689, 0.024885844748858445),\n",
       " (0.04971493728620296, 0.02485746864310148),\n",
       " (0.04965831435079729, 0.024829157175398645),\n",
       " (0.049601820250284416, 0.024800910125142208),\n",
       " (0.04954545454545456, 0.02477272727272728),\n",
       " (0.049489216799091955, 0.024744608399545977),\n",
       " (0.04943310657596372, 0.02471655328798186),\n",
       " (0.049377123442808604, 0.024688561721404302),\n",
       " (0.0493212669683258, 0.0246606334841629),\n",
       " (0.049265536723163854, 0.024632768361581927),\n",
       " (0.04920993227990969, 0.024604966139954845),\n",
       " (0.04915445321307779, 0.024577226606538896),\n",
       " (0.04909909909909911, 0.024549549549549556),\n",
       " (0.04904386951631046, 0.02452193475815523),\n",
       " (0.04898876404494384, 0.02449438202247192),\n",
       " (0.0489337822671156, 0.0244668911335578),\n",
       " (0.04887892376681613, 0.024439461883408065),\n",
       " (0.04882418812989924, 0.02441209406494962),\n",
       " (0.04921700223713646, 0.02460850111856823),\n",
       " (0.049162011173184354, 0.024581005586592177),\n",
       " (0.04910714285714284, 0.02455357142857142),\n",
       " (0.04905239687848382, 0.02452619843924191),\n",
       " (0.04899777282850781, 0.024498886414253906),\n",
       " (0.04894327030033372, 0.02447163515016686),\n",
       " (0.0488888888888889, 0.02444444444444445),\n",
       " (0.04883462819089899, 0.024417314095449495),\n",
       " (0.04878048780487805, 0.024390243902439025),\n",
       " (0.048726467331118475, 0.024363233665559238),\n",
       " (0.04867256637168143, 0.024336283185840715),\n",
       " (0.048618784530386754, 0.024309392265193377),\n",
       " (0.048565121412803516, 0.024282560706401758),\n",
       " (0.04851157662624037, 0.024255788313120186),\n",
       " (0.048898678414096924, 0.024449339207048462),\n",
       " (0.04884488448844886, 0.02442244224422443),\n",
       " (0.048791208791208796, 0.024395604395604398),\n",
       " (0.04873765093304061, 0.024368825466520305),\n",
       " (0.0486842105263158, 0.0243421052631579),\n",
       " (0.04863088718510405, 0.024315443592552024),\n",
       " (0.04857768052516414, 0.02428884026258207),\n",
       " (0.04852459016393445, 0.024262295081967224),\n",
       " (0.04847161572052401, 0.024235807860262006),\n",
       " (0.04841875681570338, 0.02420937840785169),\n",
       " (0.04836601307189543, 0.024183006535947717),\n",
       " (0.048313384113166484, 0.024156692056583242),\n",
       " (0.048260869565217406, 0.024130434782608703),\n",
       " (0.04820846905537462, 0.02410423452768731),\n",
       " (0.04815618221258133, 0.024078091106290667),\n",
       " (0.04810400866738895, 0.024052004333694477),\n",
       " (0.04805194805194804, 0.02402597402597402),\n",
       " (0.048, 0.024),\n",
       " (0.04794816414686825, 0.023974082073434124),\n",
       " (0.04789644012944985, 0.023948220064724926),\n",
       " (0.047844827586206895, 0.023922413793103448),\n",
       " (0.047793326157158235, 0.023896663078579117),\n",
       " (0.047741935483870984, 0.023870967741935492),\n",
       " (0.04769065520945222, 0.02384532760472611),\n",
       " (0.047639484978540786, 0.023819742489270393),\n",
       " (0.04758842443729905, 0.023794212218649524),\n",
       " (0.047537473233404716, 0.023768736616702358),\n",
       " (0.04748663101604281, 0.023743315508021404),\n",
       " (0.04743589743589745, 0.023717948717948724),\n",
       " (0.047385272145144076, 0.023692636072572038),\n",
       " (0.047334754797441383, 0.023667377398720692),\n",
       " (0.04771033013844517, 0.023855165069222584),\n",
       " (0.04765957446808509, 0.023829787234042544),\n",
       " (0.04760892667375134, 0.02380446333687567),\n",
       " (0.047558386411889586, 0.023779193205944793),\n",
       " (0.04750795334040299, 0.023753976670201495),\n",
       " (0.04788135593220338, 0.02394067796610169),\n",
       " (0.04783068783068783, 0.023915343915343914),\n",
       " (0.0477801268498943, 0.02389006342494715),\n",
       " (0.04772967265047519, 0.023864836325237594),\n",
       " (0.04767932489451479, 0.023839662447257395),\n",
       " (0.0476290832455216, 0.0238145416227608),\n",
       " (0.04757894736842103, 0.023789473684210517),\n",
       " (0.04752891692954786, 0.02376445846477393),\n",
       " (0.04747899159663867, 0.023739495798319334),\n",
       " (0.04742917103882478, 0.02371458551941239),\n",
       " (0.047379454926624744, 0.023689727463312372),\n",
       " (0.047329842931937184, 0.023664921465968592),\n",
       " (0.04728033472803346, 0.02364016736401673),\n",
       " (0.047230929989550675, 0.023615464994775338),\n",
       " (0.047181628392484326, 0.023590814196242163),\n",
       " (0.04713242961418143, 0.023566214807090716),\n",
       " (0.047083333333333324, 0.023541666666666662),\n",
       " (0.0470343392299688, 0.0235171696149844),\n",
       " (0.04698544698544698, 0.02349272349272349),\n",
       " (0.046936656282450695, 0.023468328141225347),\n",
       " (0.04688796680497927, 0.023443983402489633),\n",
       " (0.04683937823834197, 0.023419689119170986),\n",
       " (0.04720496894409938, 0.02360248447204969),\n",
       " (0.0471561530506722, 0.0235780765253361),\n",
       " (0.047107438016528926, 0.023553719008264463),\n",
       " (0.04747162022703817, 0.023735810113519086),\n",
       " (0.047422680412371146, 0.023711340206185573),\n",
       " (0.047373841400617914, 0.023686920700308957),\n",
       " (0.04732510288065846, 0.02366255144032923),\n",
       " (0.04768756423432681, 0.023843782117163405),\n",
       " (0.04763860369609856, 0.02381930184804928),\n",
       " (0.04758974358974358, 0.02379487179487179),\n",
       " (0.04754098360655737, 0.023770491803278684),\n",
       " (0.04749232343909929, 0.023746161719549644),\n",
       " (0.047443762781186116, 0.023721881390593058),\n",
       " (0.047803881511746665, 0.023901940755873333),\n",
       " (0.04775510204081632, 0.02387755102040816),\n",
       " (0.047706422018348606, 0.023853211009174303),\n",
       " (0.04765784114052951, 0.023828920570264756),\n",
       " (0.047609359104781306, 0.023804679552390653),\n",
       " (0.04796747967479673, 0.023983739837398366),\n",
       " (0.04791878172588833, 0.023959390862944165),\n",
       " (0.04787018255578093, 0.023935091277890466),\n",
       " (0.048226950354609915, 0.024113475177304958),\n",
       " (0.04817813765182186, 0.02408906882591093),\n",
       " (0.04812942366026288, 0.02406471183013144),\n",
       " (0.048080808080808085, 0.024040404040404043),\n",
       " (0.04803229061553985, 0.024016145307769923),\n",
       " (0.047983870967741954, 0.023991935483870977),\n",
       " (0.047935548841893244, 0.023967774420946622),\n",
       " (0.04788732394366196, 0.02394366197183098),\n",
       " (0.047839195979899524, 0.023919597989949762),\n",
       " (0.04779116465863456, 0.02389558232931728),\n",
       " (0.04774322968906719, 0.023871614844533595),\n",
       " (0.04769539078156311, 0.023847695390781555),\n",
       " (0.047647647647647645, 0.023823823823823823),\n",
       " (0.0476, 0.0238),\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng.dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "n_iters = 5000\n",
    "# Train\n",
    "for i in range(n_iters):\n",
    "    rand_number = random.random()\n",
    "    if rand_number > 0.5 :\n",
    "        #print(\"\\nProcessing 0\")\n",
    "        pred = ng.process(a.spikes, 1)\n",
    "        correct += int(pred == 1) \n",
    "        logging.info('-'*20 + f'Predicted {pred}, real {0}')\n",
    "    else:\n",
    "        #print(\"\\nProcessing 1\")\n",
    "        pred = ng.process(b.spikes, 0)\n",
    "        correct += int(pred == 0) \n",
    "        logging.info('-'*20 + f'Predicted {pred}, real {1}')\n",
    "    \n",
    "print(f'Accuracy: {correct/(n_iters)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "correct = 0\n",
    "n_iters = 2000\n",
    "for i in range(n_iters):\n",
    "    rand_number = random.random()\n",
    "    if rand_number > 0.5 :\n",
    "        #print(\"\\nProcessing 0\")\n",
    "        pred = ng.predict(a.spikes)\n",
    "        correct += int(pred == 1) \n",
    "        logging.info('-'*20 + f'Predicted {pred}, real {0}')\n",
    "    else:\n",
    "        #print(\"\\nProcessing 1\")\n",
    "        pred = ng.predict(b.spikes)\n",
    "        correct += int(pred == 0) \n",
    "        logging.info('-'*20 + f'Predicted {pred}, real {1}')\n",
    "    \n",
    "print(f'Accuracy: {correct/(n_iters)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
